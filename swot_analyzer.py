#!/usr/bin/env python3
"""
Sistema de An√°lise SWOT Individual com RAG Local
=================================================
Utiliza LangChain + ChromaDB + Ollama para gerar an√°lises SWOT 
separadas para cada perfil, garantindo isolamento de dados.

Autor: Engenharia de Dados e IA
"""

import os
import json
import glob
from datetime import datetime
from typing import List, Dict, Any

# LangChain Core & Community
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.llms import Ollama

# Configura√ß√µes
WORKSPACE_DIR = os.path.dirname(os.path.abspath(__file__))
CHROMA_PERSIST_DIR = os.path.join(WORKSPACE_DIR, "chroma_db")
REPORTS_DIR = os.path.join(WORKSPACE_DIR, "swot_reports")

# Modelos Ollama
EMBEDDING_MODEL = "nomic-embed-text"
LLM_MODEL = "qwen3:30b"  


def carregar_e_processar_jsons(diretorio: str) -> List[Document]:
    """
    ETL: Carrega todos os arquivos .json e cria objetos Document
    com page_content e metadata conforme especificado.
    
    Args:
        diretorio: Caminho do diret√≥rio contendo os arquivos .json
        
    Returns:
        Lista de objetos Document prontos para indexa√ß√£o
    """
    documentos = []
    arquivos_json = glob.glob(os.path.join(diretorio, "*.json"))
    
    print(f"\nüìÇ Encontrados {len(arquivos_json)} arquivos JSON para processar...")
    
    for arquivo_path in arquivos_json:
        nome_arquivo = os.path.basename(arquivo_path)
        print(f"  ‚îú‚îÄ‚îÄ Processando: {nome_arquivo}")
        
        try:
            with open(arquivo_path, 'r', encoding='utf-8') as f:
                dados = json.load(f)
            
            # Garante que dados seja uma lista
            if isinstance(dados, dict):
                dados = [dados]
            
            posts_processados = 0
            for post in dados:
                # Extrai campos com tratamento de valores None/vazios
                caption = post.get('caption', '') or ''
                alt = post.get('alt', '') or ''
                timestamp = post.get('timestamp', '') or ''
                likes = post.get('likesCount', 0)
                if likes == -1:  # Valor sentinela no JSON
                    likes = 0
                
                # Processa coment√°rios
                comentarios_formatados = []
                latest_comments = post.get('latestComments', []) or []
                for comentario in latest_comments:
                    if isinstance(comentario, dict):
                        user = comentario.get('ownerUsername', 'An√¥nimo')
                        texto = comentario.get('text', '')
                        if texto:
                            comentarios_formatados.append(f"{user}: {texto}")
                
                comentarios_str = "\n".join(comentarios_formatados) if comentarios_formatados else "Sem coment√°rios"
                
                # Monta o page_content conforme especifica√ß√£o
                page_content = f"""
PERFIL/FONTE: {nome_arquivo}
---
LEGENDA DO POST:
{caption}
---
DESCRI√á√ÉO DA IMAGEM (ALT):
{alt}
---
COMENT√ÅRIOS:
{comentarios_str}
""".strip()
                
                # Cria o Document com metadata para filtragem
                doc = Document(
                    page_content=page_content,
                    metadata={
                        "source": nome_arquivo,  # CR√çTICO: usado para filtro
                        "timestamp": timestamp,
                        "likes": likes,
                        "post_id": post.get('id', ''),
                        "type": post.get('type', 'Unknown')
                    }
                )
                documentos.append(doc)
                posts_processados += 1
            
            print(f"  ‚îÇ   ‚îî‚îÄ‚îÄ {posts_processados} posts extra√≠dos")
            
        except json.JSONDecodeError as e:
            print(f"  ‚îÇ   ‚ö†Ô∏è  Erro ao decodificar JSON: {e}")
        except Exception as e:
            print(f"  ‚îÇ   ‚ö†Ô∏è  Erro inesperado: {e}")
    
    print(f"\n‚úÖ Total de documentos criados: {len(documentos)}")
    return documentos


def criar_vector_store(documentos: List[Document], persist_directory: str) -> Chroma:
    """
    Cria ou carrega o Vector Store ChromaDB com embeddings Ollama.
    
    Args:
        documentos: Lista de Documents para indexar
        persist_directory: Diret√≥rio para persist√™ncia do ChromaDB
        
    Returns:
        Inst√¢ncia do ChromaDB
    """
    print("\nüîÑ Inicializando embeddings com Ollama (nomic-embed-text)...")
    
    embeddings = OllamaEmbeddings(
        model=EMBEDDING_MODEL,
        base_url="http://localhost:11434"
    )
    
    # Remove banco antigo para garantir dados atualizados
    if os.path.exists(persist_directory):
        import shutil
        print("  ‚îú‚îÄ‚îÄ Removendo banco de vetores antigo...")
        shutil.rmtree(persist_directory)
    
    print("  ‚îú‚îÄ‚îÄ Criando novo banco de vetores...")
    print(f"  ‚îú‚îÄ‚îÄ Indexando {len(documentos)} documentos...")
    
    vector_store = Chroma.from_documents(
        documents=documentos,
        embedding=embeddings,
        persist_directory=persist_directory,
        collection_name="perfis_swot"
    )
    
    print("  ‚îî‚îÄ‚îÄ ‚úÖ Vector Store criado com sucesso!")
    return vector_store


def gerar_swot_individual(
    vector_store: Chroma,
    nome_arquivo_alvo: str,
    k: int = 15
) -> str:
    """
    Gera an√°lise SWOT para um perfil espec√≠fico usando filtro de metadados.
    
    CRITICAL: O filtro {"source": nome_arquivo_alvo} garante que apenas
    documentos do perfil especificado sejam considerados na an√°lise.
    
    Args:
        vector_store: Inst√¢ncia do ChromaDB
        nome_arquivo_alvo: Nome do arquivo (ex: "reitor.json")
        k: N√∫mero de documentos a recuperar (default: 15)
        
    Returns:
        Texto da an√°lise SWOT gerada
    """
    print(f"\nüéØ Gerando SWOT para: {nome_arquivo_alvo}")
    print(f"  ‚îú‚îÄ‚îÄ Configurando filtro de metadados: source = {nome_arquivo_alvo}")
    print(f"  ‚îú‚îÄ‚îÄ Recuperando top {k} documentos relevantes...")
    
    # Configura o retriever com FILTRO DE METADADOS - CR√çTICO!
    retriever = vector_store.as_retriever(
        search_type="similarity",
        search_kwargs={
            "k": k,
            "filter": {"source": nome_arquivo_alvo}  # ISOLAMENTO DE DADOS
        }
    )
    
    # Verifica se h√° documentos para o perfil
    docs_teste = retriever.invoke("an√°lise geral do perfil")
    if not docs_teste:
        return f"‚ö†Ô∏è Nenhum documento encontrado para o perfil: {nome_arquivo_alvo}"
    
    print(f"  ‚îú‚îÄ‚îÄ {len(docs_teste)} documentos recuperados para an√°lise")
    
    # Configura o LLM Ollama
    llm = Ollama(
        model=LLM_MODEL,
        base_url="http://localhost:11434",
        temperature=0.3,  # Menor temperatura para an√°lises mais focadas
        num_ctx=8192,     # Contexto expandido para an√°lises longas
    )
    
    # Prompt especializado para an√°lise SWOT
    prompt_template = """Voc√™ √© um analista estrat√©gico especializado em an√°lise de perfis p√∫blicos e institucionais.

CONTEXTO DOS DOCUMENTOS:
{context}

TAREFA:
Baseado EXCLUSIVAMENTE nos documentos fornecidos acima, crie uma An√°lise SWOT detalhada e profunda.

INSTRU√á√ïES ESPEC√çFICAS:
1. **FOR√áAS (Strengths)**: Identifique pontos fortes. CITE frases espec√≠ficas dos coment√°rios ou legendas como evid√™ncia.
2. **FRAQUEZAS (Weaknesses)**: Identifique pontos fracos ou √°reas de melhoria. CITE frases que demonstrem cr√≠ticas ou problemas.
3. **OPORTUNIDADES (Opportunities)**: Identifique oportunidades de crescimento baseadas nos padr√µes das legendas e engajamento.
4. **AMEA√áAS (Threats)**: Identifique potenciais riscos ou amea√ßas baseadas nas intera√ß√µes e contexto.

FORMATO DE RESPOSTA:
Use o formato abaixo, sendo detalhado e citando evid√™ncias textuais:

## üìä AN√ÅLISE SWOT

### üí™ FOR√áAS (Strengths)
[Liste cada for√ßa com cita√ß√£o de evid√™ncia]

### ‚ö†Ô∏è FRAQUEZAS (Weaknesses)  
[Liste cada fraqueza com cita√ß√£o de evid√™ncia]

### üöÄ OPORTUNIDADES (Opportunities)
[Liste cada oportunidade identificada]

### üî• AMEA√áAS (Threats)
[Liste cada amea√ßa identificada]

### üìà PADR√ïES IDENTIFICADOS NAS LEGENDAS
[Analise temas recorrentes, tom de comunica√ß√£o, estrat√©gias]

### üí° RECOMENDA√á√ïES ESTRAT√âGICAS
[Baseado na an√°lise, sugira 3-5 a√ß√µes concretas]

Resposta detalhada:"""

    prompt = PromptTemplate(
        template=prompt_template,
        input_variables=["context"]
    )
    
    # Fun√ß√£o para formatar documentos
    def format_docs(docs):
        return "\n\n---\n\n".join(doc.page_content for doc in docs)
    
    # Cria a chain usando LCEL (LangChain Expression Language)
    rag_chain = (
        {"context": retriever | format_docs}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    # Executa a an√°lise
    print("  ‚îú‚îÄ‚îÄ Executando an√°lise com LLM (llama3)...")
    
    query = f"Realize uma an√°lise SWOT completa e aprofundada do perfil '{nome_arquivo_alvo}'."
    
    resultado = rag_chain.invoke(query)
    
    print("  ‚îî‚îÄ‚îÄ ‚úÖ An√°lise conclu√≠da!")
    
    return resultado


def salvar_relatorio(nome_arquivo: str, conteudo: str, diretorio_saida: str) -> str:
    """
    Salva o relat√≥rio SWOT em arquivo Markdown.
    
    Args:
        nome_arquivo: Nome do perfil analisado
        conteudo: Conte√∫do da an√°lise SWOT
        diretorio_saida: Diret√≥rio para salvar os relat√≥rios
        
    Returns:
        Caminho do arquivo salvo
    """
    os.makedirs(diretorio_saida, exist_ok=True)
    
    nome_base = nome_arquivo.replace('.json', '')
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    nome_relatorio = f"SWOT_{nome_base}_{timestamp}.md"
    caminho_completo = os.path.join(diretorio_saida, nome_relatorio)
    
    # Monta o relat√≥rio completo
    relatorio = f"""# Relat√≥rio de An√°lise SWOT
## Perfil: {nome_arquivo}
**Data de Gera√ß√£o:** {datetime.now().strftime("%d/%m/%Y √†s %H:%M:%S")}
**Modelo LLM:** {LLM_MODEL}
**Modelo de Embeddings:** {EMBEDDING_MODEL}

---

{conteudo}

---
*Relat√≥rio gerado automaticamente pelo Sistema de An√°lise SWOT com RAG Local*
"""
    
    with open(caminho_completo, 'w', encoding='utf-8') as f:
        f.write(relatorio)
    
    return caminho_completo


def main():
    """
    Fun√ß√£o principal que orquestra todo o pipeline de an√°lise SWOT.
    """
    print("=" * 60)
    print("üî¨ SISTEMA DE AN√ÅLISE SWOT COM RAG LOCAL")
    print("=" * 60)
    print(f"üìÖ Execu√ß√£o iniciada em: {datetime.now().strftime('%d/%m/%Y √†s %H:%M:%S')}")
    print(f"üìÇ Diret√≥rio de trabalho: {WORKSPACE_DIR}")
    
    # ========================================
    # 1. ETL - Carregamento e Processamento
    # ========================================
    print("\n" + "=" * 60)
    print("üì• ETAPA 1: CARREGAMENTO E PROCESSAMENTO DE DADOS (ETL)")
    print("=" * 60)
    
    documentos = carregar_e_processar_jsons(WORKSPACE_DIR)
    
    if not documentos:
        print("‚ùå Nenhum documento foi carregado. Verifique os arquivos JSON.")
        return
    
    # ========================================
    # 2. Indexa√ß√£o no Vector Store
    # ========================================
    print("\n" + "=" * 60)
    print("üóÑÔ∏è ETAPA 2: INDEXA√á√ÉO NO VECTOR STORE (CHROMADB)")
    print("=" * 60)
    
    vector_store = criar_vector_store(documentos, CHROMA_PERSIST_DIR)
    
    # ========================================
    # 3. Gera√ß√£o de An√°lises SWOT
    # ========================================
    print("\n" + "=" * 60)
    print("üìä ETAPA 3: GERA√á√ÉO DE AN√ÅLISES SWOT INDIVIDUAIS")
    print("=" * 60)
    
    # Lista de perfis a analisar (CONFIGUR√ÅVEL)
    # Adicione ou remova arquivos conforme necess√°rio
    perfis_para_analisar = [
        "reitor.json",
        "joserodrigues.json",
        # "sintuff.json",
        # "fabiopassos.json",
        # Adicione mais perfis aqui conforme necess√°rio
    ]
    
    # Verifica quais perfis existem
    arquivos_disponiveis = set(os.path.basename(f) for f in glob.glob(os.path.join(WORKSPACE_DIR, "*.json")))
    
    print(f"\nüìã Perfis configurados para an√°lise: {len(perfis_para_analisar)}")
    for perfil in perfis_para_analisar:
        status = "‚úÖ" if perfil in arquivos_disponiveis else "‚ùå (n√£o encontrado)"
        print(f"  ‚îú‚îÄ‚îÄ {perfil} {status}")
    
    # Processa cada perfil sequencialmente
    relatorios_gerados = []
    
    for i, perfil in enumerate(perfis_para_analisar, 1):
        if perfil not in arquivos_disponiveis:
            print(f"\n‚ö†Ô∏è Pulando {perfil}: arquivo n√£o encontrado")
            continue
            
        print(f"\n{'‚îÄ' * 50}")
        print(f"üìå Processando perfil {i}/{len(perfis_para_analisar)}: {perfil}")
        print('‚îÄ' * 50)
        
        try:
            # Gera a an√°lise SWOT com isolamento de dados
            analise = gerar_swot_individual(
                vector_store=vector_store,
                nome_arquivo_alvo=perfil,
                k=15  # Recupera bastante contexto
            )
            
            # Salva o relat√≥rio
            caminho_relatorio = salvar_relatorio(
                nome_arquivo=perfil,
                conteudo=analise,
                diretorio_saida=REPORTS_DIR
            )
            
            relatorios_gerados.append(caminho_relatorio)
            print(f"  üìÑ Relat√≥rio salvo: {caminho_relatorio}")
            
        except Exception as e:
            print(f"  ‚ùå Erro ao processar {perfil}: {e}")
            import traceback
            traceback.print_exc()
    
    # ========================================
    # 4. Resumo Final
    # ========================================
    print("\n" + "=" * 60)
    print("‚úÖ EXECU√á√ÉO CONCLU√çDA")
    print("=" * 60)
    print(f"üìä Total de relat√≥rios gerados: {len(relatorios_gerados)}")
    print(f"üìÇ Diret√≥rio dos relat√≥rios: {REPORTS_DIR}")
    print("\nüìã Relat√≥rios gerados:")
    for relatorio in relatorios_gerados:
        print(f"  ‚îî‚îÄ‚îÄ {os.path.basename(relatorio)}")
    
    print(f"\n‚è±Ô∏è Execu√ß√£o finalizada em: {datetime.now().strftime('%d/%m/%Y √†s %H:%M:%S')}")


if __name__ == "__main__":
    main()
